================================================================================
ALL PYTHON FILES IN PROJECT
================================================================================


============================================================
FILE 1/12: adaptModel.py
============================================================

import torch
import xarray as xr
import os
from torch_geometric.loader import DataLoader
from torch.utils.data import Subset
import copy

# Import your project modules
from embed_utils import add_time_embeddings, KoppenEmbedding

from dataLoader import main_dataloader
from graphBuilder import build_spatial_graph
from featurePreprocessor import prepare_model_input
from dataset import WeatherGraphDataset
from model import STGCN

# Configurable parameters
YEAR = 2021
REGION_BOUNDS = (-40, -35, 140, 145)
MODEL_PATH = "./Out_Data/SavedModels/maml_model_multivar_(LongerWindowSize&LRRate).pt"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# 1. Load region data
lat_min, lat_max, lon_min, lon_max = REGION_BOUNDS
ds, koppen_code, _ = main_dataloader(lat_min, lat_max, lon_min, lon_max)

# Ensure time embedding is present
if "day_of_year_sin" not in ds:
    ds = add_time_embeddings(ds)

# 2. Filter to one year using 'valid_time' coordinate (common in weather datasets)
if "valid_time" in ds.coords:
    # If valid_time is datetime64 type, use .dt.year
    try:
        ds = ds.sel(valid_time=ds["valid_time"].dt.year == YEAR)
    except Exception as e:
        # In case 'valid_time' is not datetime64, show diagnostic info
        print("Error filtering by year, 'valid_time' type:", ds["valid_time"].dtype)
        raise
else:
    print("No 'valid_time' coordinate found. Available coords:", list(ds.coords))
    raise KeyError("Did not find 'valid_time' for time filtering.")

# 3. Build graph, features, windowed dataset
edge_index, num_nodes, _ = build_spatial_graph(ds, k_neighbors=4)
koppen_embed_dim = 8
koppen_embed = KoppenEmbedding(embedding_dim=koppen_embed_dim).to(DEVICE)
features, stats = prepare_model_input(ds, koppen_code, koppen_embed, normalize=True)

WINDOW_SIZE = 24
FORECAST_HORIZON = 8
dataset = WeatherGraphDataset(
    features, edge_index, window_size=WINDOW_SIZE, forecast_horizon=FORECAST_HORIZON
)

# 4. Chronological split for adaptation
n = len(dataset)
split = int(0.8 * n)
support_idx = list(range(0, split))
query_idx = list(range(split, n))
support_ds = Subset(dataset, support_idx)
query_ds = Subset(dataset, query_idx)

# 5. Load meta-trained model and embedding
checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)
model = STGCN(
    in_channels=checkpoint["config"]["input_channels"],
    hidden_channels=checkpoint["config"]["hidden_channels"],
    out_channels=checkpoint["config"]["output_channels"],
    window_size=checkpoint["config"]["window_size"],
    forecast_horizon=checkpoint["config"]["forecast_horizon"],
    dropout_rate=0.3,
).to(DEVICE)
koppen_embed = KoppenEmbedding(embedding_dim=koppen_embed_dim).to(DEVICE)
model.load_state_dict(checkpoint["model_state_dict"])
koppen_embed.load_state_dict(checkpoint["koppen_embed_state_dict"])
model.eval()
koppen_embed.eval()

# 6. Adaptation (inner loop) on support set
inner_epochs = 3
inner_lr = 0.005
weight_decay = checkpoint["config"]["weight_decay"]
max_grad_norm = checkpoint["config"]["max_grad_norm"]

temp_model = copy.deepcopy(model)
temp_koppen = copy.deepcopy(koppen_embed)
temp_model.train()
temp_koppen.train()
optimizer = torch.optim.SGD(
    list(temp_model.parameters()) + list(temp_koppen.parameters()),
    lr=inner_lr,
    weight_decay=weight_decay,
)
criterion = torch.nn.MSELoss()
support_loader = DataLoader(
    support_ds,
    batch_size=1,
    shuffle=False,
    num_workers=0,
    pin_memory=(DEVICE == "cuda"),
)

for epoch in range(inner_epochs):
    for batch in support_loader:
        batch = batch.to(DEVICE)
        optimizer.zero_grad()
        out = temp_model(batch.x, batch.edge_index)
        loss = criterion(out, batch.y)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(
            list(temp_model.parameters()) + list(temp_koppen.parameters()),
            max_grad_norm,
        )
        optimizer.step()

# 7. Evaluation (MSE on query set‚Äîno further training)
temp_model.eval()
temp_koppen.eval()
query_loader = DataLoader(
    query_ds, batch_size=1, shuffle=False, num_workers=0, pin_memory=(DEVICE == "cuda")
)
total_loss = 0.0
n_batches = 0

with torch.no_grad():
    for batch in query_loader:
        batch = batch.to(DEVICE)
        out = temp_model(batch.x, batch.edge_index)
        loss = criterion(out, batch.y)
        total_loss += loss.item()
        n_batches += 1

avg_query_loss = total_loss / n_batches if n_batches > 0 else float("nan")
print(
    f"Meta-adapted model's query MSE on {YEAR} ({REGION_BOUNDS}): {avg_query_loss:.4f}"
)

save_dir = "./Out_Data/AdaptedModels"
os.makedirs(save_dir, exist_ok=True)
save_path = os.path.join(save_dir, f"adapted_model_{YEAR}_{REGION_BOUNDS}.pt")
torch.save(
    {
        "model_state_dict": temp_model.state_dict(),
        "koppen_embed_state_dict": temp_koppen.state_dict(),
        "year": YEAR,
        "region": REGION_BOUNDS,
        "stats": stats,  # Optional: save normalization stats
        "support_size": len(support_ds),
        "query_size": len(query_ds),
        "config": checkpoint["config"],
    },
    save_path,
)
print(f"Adapted model saved to: {save_path}")


######################################## END OF adaptModel.py ########################################


============================================================
FILE 2/12: dataLoader.py
============================================================

import os
import xarray as xr
import numpy as np
from collections import Counter

YEAR = ["2020", "2021", "2022", "2023", "2024"]
DATASET_ROOT = "E:/Study/5th Sem Mini Project/Datasets"
QUARTERS = ["Jan2Mar", "Apr2Jun", "Jul2Sept", "Oct2Dec"]
NC_FILENAMES = [
    "data_stream-oper_stepType-accum.nc",
    "data_stream-oper_stepType-instant.nc",
]


def to_0360(lon):
    return lon if lon >= 0 else lon + 360


def load_region_data(lat_min, lat_max, lon_min, lon_max, out_nc_path=None):
    lon_max = to_0360(lon_max)
    lon_min = to_0360(lon_min)

    def slice_dim(ds, dim, start, stop):
        coords = ds[dim].values
        if coords[0] > coords[-1]:
            return ds.sel({dim: slice(stop, start)})
        else:
            return ds.sel({dim: slice(start, stop)})

    all_var_datasets = []
    for year in YEAR:
        for quarter in QUARTERS:
            file_datasets = []
            for fname in NC_FILENAMES:
                fpath = os.path.join(DATASET_ROOT, year, quarter, fname)
                print(
                    f"Loading {fpath} for region latitude:[{lat_min},{lat_max}] longitude:[{lon_min},{lon_max}]"
                )
                ds = xr.open_dataset(fpath)
                ds_sel = ds.pipe(slice_dim, "latitude", lat_min, lat_max)
                ds_sel = ds_sel.pipe(slice_dim, "longitude", lon_min, lon_max)
                ds_sel = ds_sel.drop_vars("expver", errors="ignore")
                file_datasets.append(ds_sel)
            quarter_combined = xr.merge(file_datasets, compat="override")
            all_var_datasets.append(quarter_combined)

    data_combined = xr.concat(all_var_datasets, dim="valid_time").sortby("valid_time")
    print("Data for region combined shape:", data_combined.sizes)
    if out_nc_path is not None:
        print(f"Saving combined dataset to: {out_nc_path}")
        data_combined.to_netcdf(out_nc_path)
    return data_combined


META_TRAIN_REGIONS = [
    (-9.5, -4.5, -67.5, -62.5),
    (12.5, 17.5, 102.5, 107.5),
    (22.5, 27.5, 19.5, 24.5),
    (-23.5, -18.5, 132.5, 137.5),
    (43.5, 48.5, 7.5, 12.5),
    (35.5, 40.5, -5.5, -0.5),
    (53.5, 58.5, 34.5, 39.5),
    (44.5, 49.5, 125.5, 130.5),
    (67.5, 72.5, -32.5, -27.5),
    (-20, -15, -70, -65),
    (32.5, 37.5, 137.5, 142.5),
    (-35.5, -30.5, 16.5, 21.5),
    (51.5, 56.5, -112.5, -107.5),
    (29.5, 34.5, -101.5, -96.5),
    (11.5, 16.5, 86.5, 91.5),
]

# Koppen codes and corresponding classes from your metadata
code_to_class = {
    1: "Af",
    2: "Am",
    3: "Aw",
    4: "BSh",
    5: "BSk",
    6: "BWh",
    7: "BWk",
    8: "Cfa",
    9: "Cfb",
    10: "Cfc",
    11: "Csa",
    12: "Csb",
    13: "Csc",
    14: "Cwa",
    15: "Cwb",
    16: "Cwc",
    17: "Dfa",
    18: "Dfb",
    19: "Dfc",
    20: "Dfd",
    21: "Dsa",
    22: "Dsb",
    23: "Dsc",
    24: "Dsd",
    25: "Dwa",
    26: "Dwb",
    27: "Dwc",
    28: "Dwd",
    29: "EF",
    30: "ET",
}


def get_koppen_class(lat_min, lat_max, lon_min, lon_max):
    ds = xr.open_dataset("E:/Study/5th Sem Mini Project/Datasets/RobustKGMaps.nc")

    def slice_dim(ds, dim, start, stop):
        coords = ds[dim].values
        if coords[0] > coords[-1]:
            return ds.sel({dim: slice(stop, start)})
        else:
            return ds.sel({dim: slice(start, stop)})

    ds_sel = ds.pipe(slice_dim, "lat", lat_min, lat_max)
    ds_sel = ds_sel.pipe(slice_dim, "lon", lon_min, lon_max)

    koppen_data = ds_sel["MasterMap1"].values.flatten()

    koppen_data = koppen_data[~np.isnan(koppen_data)].astype(int)

    if len(koppen_data) == 0:
        return -1  # No data in region

    counts = Counter(koppen_data)
    majority_code = counts.most_common(1)[0][0]

    return majority_code


def main_dataloader(lat_min, lat_max, lon_min, lon_max):
    out_nc_path = f"E:/Study/5th Sem Mini Project/Code/Out_Data/lat{lat_min}-{lat_max}_lon{lon_min}-{lon_max}.nc"
    return (
        load_region_data(lat_min, lat_max, lon_min, lon_max, out_nc_path=out_nc_path),
        get_koppen_class(lat_min, lat_max, lon_min, lon_max),
        out_nc_path,
    )


######################################## END OF dataLoader.py ########################################


============================================================
FILE 3/12: dataset.py
============================================================

import torch
from torch.utils.data import Dataset
from torch_geometric.data import Data


class WeatherGraphDataset(Dataset):
    """
    Multi-variable forecasting dataset for multi-step prediction.
    Predicts all weather variables for forecast_horizon steps ahead.
    """

    def __init__(
        self,
        features,
        edge_index,
        window_size=6,
        forecast_horizon=1,
    ):
        self.features = features
        self.edge_index = edge_index
        self.window_size = window_size
        self.forecast_horizon = forecast_horizon
        self.num_weather_vars = 12
        self.num_nodes = features.shape[1]
        self.valid_indices = range(window_size, len(features) - forecast_horizon)

    def __len__(self):
        return len(self.valid_indices)

    def __getitem__(self, idx):
        actual_idx = self.valid_indices[idx]

        # Input window: Past window_size timesteps
        start = actual_idx - self.window_size
        end = actual_idx
        window = self.features[start:end]  # [window_size, num_nodes, 24]
        x = window.reshape(self.window_size * self.num_nodes, -1)

        # Multi-step targets
        targets = []
        for h in range(1, self.forecast_horizon + 1):
            future_idx = actual_idx + h
            target = self.features[future_idx, :, : self.num_weather_vars]
            targets.append(target)
        targets = torch.stack(targets, dim=0)  # [forecast_horizon, num_nodes, 12]
        y = targets.reshape(
            self.forecast_horizon * self.num_nodes, self.num_weather_vars
        )

        return Data(
            x=x.clone().detach(),
            edge_index=self.edge_index,
            y=y.clone().detach(),
        )


######################################## END OF dataset.py ########################################


============================================================
FILE 4/12: embed_utils.py
============================================================

# embed_utils.py

import torch
import torch.nn as nn
import xarray as xr
import numpy as np
import pandas as pd


def add_time_embeddings(ds: xr.Dataset) -> xr.Dataset:
    time_dim = "time" if "time" in ds.dims else "valid_time"
    times = pd.to_datetime(ds[time_dim].values)
    day_of_year = times.dayofyear.values
    time_of_day = (
        times.hour.values + times.minute.values / 60 + times.second.values / 3600
    )

    year_progress = 2 * np.pi * day_of_year / 365.25
    day_progress = 2 * np.pi * time_of_day / 24.0

    ds = ds.assign(
        year_progress_sin=(time_dim, np.sin(year_progress)),
        year_progress_cos=(time_dim, np.cos(year_progress)),
        day_progress_sin=(time_dim, np.sin(day_progress)),
        day_progress_cos=(time_dim, np.cos(day_progress)),
    )
    return ds


class KoppenEmbedding(nn.Module):
    def __init__(self, embedding_dim=8):
        super(KoppenEmbedding, self).__init__()
        self.num_classes = 31  # indices 0-30, where 0 is unused/padding
        self.embedding_dim = embedding_dim
        self.embedding = nn.Embedding(self.num_classes, embedding_dim)

    def forward(self, koppen_codes):
        return self.embedding(koppen_codes)


######################################## END OF embed_utils.py ########################################


============================================================
FILE 5/12: featurePreprocessor.py
============================================================

"""
Feature extraction and preprocessing for weather data.
"""

import torch
import numpy as np
from embed_utils import add_time_embeddings, KoppenEmbedding
import xarray as xr


def diagnose_nan_percentage(ds):
    """Print NaN percentage for each variable."""
    print("\n" + "=" * 60)
    print("NaN PERCENTAGE BY VARIABLE:")
    print("=" * 60)
    all_vars = [
        "u10",
        "v10",
        "d2m",
        "t2m",
        "sp",
        "tp",
        "u100",
        "v100",
        "str",
        "hcc",
        "lcc",
        "mcc",
        "e",
    ]

    for var in all_vars:
        if var in ds:
            data = ds[var].values
            nan_pct = (np.isnan(data).sum() / data.size) * 100
            status = "‚úÖ" if nan_pct < 5 else "‚ö†Ô∏è" if nan_pct < 15 else "‚ùå"
            print(f"{status} {var:6s}: {nan_pct:5.1f}% NaN")
    print("=" * 60 + "\n")


# ‚úÖ CLEANED: Only variables with low NaN percentage
WEATHER_VARS = [
    "u10",
    "v10",
    "t2m",
    "d2m",
    "sp",
    "tp",
    "u100",
    "v100",
    "str",
    "hcc",
    "lcc",
    "e",
]
# Total: 12 weather variables
# Input features: 12 (weather) + 4 (time) + 8 (K√∂ppen) = 24 total

TIME_VARS = [
    "year_progress_sin",
    "year_progress_cos",
    "day_progress_sin",
    "day_progress_cos",
]


def prepare_model_input(ds, koppen_code, koppen_embed_layer, normalize=True):
    """
    Extract and prepare all features for ST-GCN input.
    HANDLES NaN VALUES in weather data.

    Args:
        ds: xarray Dataset (from dataLoader.main_dataloader)
        koppen_code: Integer K√∂ppen code
        koppen_embed_layer: KoppenEmbedding instance
        normalize: Whether to normalize weather features

    Returns:
        features: Tensor [time, nodes, total_features]
        stats: Normalization statistics dict
    """

    # Extract weather data
    weather_data = np.stack([ds[var].values for var in WEATHER_VARS], axis=-1)
    # Shape: [time, lat, lon, ... (12 variables)]

    # ‚úÖ DIAGNOSTIC 1: PRINT VARIABLE ORDER
    print("\n" + "=" * 60)
    print("VARIABLE ORDER IN FEATURES:")
    print("=" * 60)
    for i, var in enumerate(WEATHER_VARS):
        print(f"Index {i}: {var}")
    print("=" * 60 + "\n")

    # ‚úÖ CHECK FOR NaN
    nan_count = np.isnan(weather_data).sum()
    if nan_count > 0:
        print(
            f"      ‚ö†Ô∏è  Found {nan_count} NaN values in weather data. Filling with means..."
        )

        # Fill NaN with column-wise mean (per variable)
        for i in range(weather_data.shape[-1]):
            var_data = weather_data[..., i]
            var_mean = np.nanmean(var_data)  # Mean ignoring NaN
            if np.isnan(var_mean):
                var_mean = 0.0  # If all NaN, use 0
            weather_data[..., i] = np.nan_to_num(var_data, nan=var_mean)

        print(f"      ‚úÖ NaN values filled")

    # Extract time data
    time_data = np.stack([ds[var].values for var in TIME_VARS], axis=-1)
    # Shape: [time, 4]

    # Get dimensions
    num_time, num_lat, num_lon, num_weather = weather_data.shape
    num_nodes = num_lat * num_lon

    # Reshape weather: [time, lat, lon, 12] -> [time, nodes, 12]
    weather_features = weather_data.reshape(num_time, num_nodes, num_weather)

    # Normalize weather features
    stats = {}
    if normalize:
        mean = weather_features.mean(axis=(0, 1))
        std = (
            weather_features.std(axis=(0, 1)) + 1e-8
        )  # Add epsilon to avoid div by zero

        # Check for problematic values
        if np.any(np.isnan(mean)) or np.any(np.isnan(std)):
            print(f"      ‚ö†Ô∏è  NaN in statistics! Using safe defaults.")
            mean = np.nan_to_num(mean, nan=0.0)
            std = np.nan_to_num(std, nan=1.0)

        weather_features = (weather_features - mean) / std
        stats = {"mean": mean, "std": std}

        # ‚úÖ DIAGNOSTIC 2: PRINT NORMALIZATION STATS
        print("\n" + "=" * 60)
        print("NORMALIZATION STATS:")
        print("=" * 60)
        for i, var in enumerate(WEATHER_VARS):
            print(f"{var:6s}: mean={mean[i]:8.2f}, std={std[i]:8.2f}")
            if var == "t2m":
                print(f"  üëÜ t2m normalized: (X - {mean[i]:.2f}) / {std[i]:.2f}")
        print("=" * 60 + "\n")

    # Convert to tensors
    weather_tensor = torch.tensor(weather_features, dtype=torch.float32)

    # Expand time features to all nodes
    time_expanded = np.tile(time_data[:, np.newaxis, :], (1, num_nodes, 1))
    time_tensor = torch.tensor(time_expanded, dtype=torch.float32)

    # Get K√∂ppen embeddings
    device = next(koppen_embed_layer.parameters()).device
    koppen_tensor = torch.tensor([koppen_code], dtype=torch.long).to(device)
    koppen_embed = koppen_embed_layer(koppen_tensor)
    koppen_embed = koppen_embed.cpu()
    koppen_expanded = koppen_embed.unsqueeze(0).expand(num_time, num_nodes, -1)

    # Concatenate
    combined = torch.cat([weather_tensor, time_tensor, koppen_expanded], dim=-1)

    # Final NaN check
    if torch.isnan(combined).any():
        print(f"      ‚ö†Ô∏è  WARNING: Still have NaN after processing!")
        combined = torch.nan_to_num(combined, nan=0.0)

    return combined, stats


def denormalize_predictions(
    predictions, stats, target_var_idx=2
):  # ‚úÖ CHANGED from 3 to 2!
    """
    Denormalize predictions back to original scale.

    Args:
        predictions: Normalized predictions (tensor or numpy array)
        stats: Dict with 'mean' and 'std' from prepare_model_input
        target_var_idx: Index of target variable (default 2 = t2m in current WEATHER_VARS)

    Returns:
        Denormalized predictions in original units
    """
    if "mean" in stats and "std" in stats:
        mean = stats["mean"][target_var_idx]  # Get mean for target variable
        std = stats["std"][target_var_idx]  # Get std for target variable

        # Convert to same type as predictions
        if isinstance(predictions, torch.Tensor):
            mean = torch.tensor(
                mean, dtype=predictions.dtype, device=predictions.device
            )
            std = torch.tensor(std, dtype=predictions.dtype, device=predictions.device)

        return predictions * std + mean

    return predictions


def denormalize_all_predictions(predictions, stats):
    """
    Denormalize ALL 12 weather variables.

    Args:
        predictions: [samples, 12] or [12] numpy array
        stats: dict with 'mean' and 'std' arrays of shape [12]

    Returns:
        Denormalized predictions in same shape
    """
    mean = stats["mean"]
    std = stats["std"]

    # predictions shape: [samples, 12] or [12]
    if predictions.ndim == 1:
        # Single sample
        denorm = predictions * std + mean
    else:
        # Multiple samples
        denorm = predictions * std[np.newaxis, :] + mean[np.newaxis, :]

    return denorm


######################################## END OF featurePreprocessor.py ########################################


============================================================
FILE 6/12: file_list.py
============================================================

import os
import glob


def collect_python_files():
    """
    Collect all .py files in current directory and combine their contents
    into a single text file for sharing/analysis.
    """

    # Find all Python files in current directory
    python_files = glob.glob("*.py")
    python_files.sort()

    if not python_files:
        print("No Python files found in current directory!")
        return

    print(f"Found {len(python_files)} Python files:")
    for f in python_files:
        print(f"  - {f}")

    # Create combined output file
    output_file = "all_python_code.txt"

    with open(output_file, "w", encoding="utf-8") as outfile:
        outfile.write("=" * 80 + "\n")
        outfile.write("ALL PYTHON FILES IN PROJECT\n")
        outfile.write("=" * 80 + "\n\n")

        for i, py_file in enumerate(python_files, 1):
            # Write header for each file
            outfile.write(f"\n{'='*60}\n")
            outfile.write(f"FILE {i}/{len(python_files)}: {py_file}\n")
            outfile.write(f"{'='*60}\n\n")

            try:
                # Read and write file contents
                with open(py_file, "r", encoding="utf-8") as infile:
                    content = infile.read()
                    outfile.write(content)

                # Add separator
                outfile.write(f"\n\n{'#'*40} END OF {py_file} {'#'*40}\n\n")

            except Exception as e:
                outfile.write(f"ERROR: Could not read {py_file}: {e}\n\n")

    print(f"\n‚úÖ All Python files combined into: {output_file}")
    print(f"You can now open {output_file} and copy-paste its contents!")


if __name__ == "__main__":
    collect_python_files()


######################################## END OF file_list.py ########################################


============================================================
FILE 7/12: graphBuilder.py
============================================================

"""
Graph construction utilities for spatial weather data.
"""
import torch
import numpy as np
from scipy.spatial import cKDTree


def build_spatial_graph(ds, k_neighbors=4):
    """
    Build k-NN spatial graph from lat/lon coordinates.
    
    Args:
        ds: xarray Dataset with 'latitude' and 'longitude' coordinates
        k_neighbors: Number of nearest neighbors to connect
    
    Returns:
        edge_index: Tensor of shape [2, num_edges] for PyTorch Geometric
        num_nodes: Total number of spatial nodes
        node_positions: Array of [lat, lon] positions
    """
    # Get lat/lon grids
    lats = ds.latitude.values
    lons = ds.longitude.values
    
    # Create meshgrid
    lat_grid, lon_grid = np.meshgrid(lats, lons, indexing='ij')
    
    # Flatten to get node positions
    node_positions = np.c_[lat_grid.ravel(), lon_grid.ravel()]
    num_nodes = len(node_positions)
    
    # Build k-NN graph using KDTree
    tree = cKDTree(node_positions)
    _, neighbors = tree.query(node_positions, k=k_neighbors+1)  # +1 includes self
    
    # Create edge list
    edges = []
    for node_idx, neighbor_indices in enumerate(neighbors):
        for neighbor_idx in neighbor_indices[1:]:  # Skip self-connection
            edges.append([node_idx, neighbor_idx])
    
    # Convert to tensor format [2, num_edges]
    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()
    
    print(f"Graph created: {num_nodes} nodes, {edge_index.shape[1]} edges")
    return edge_index, num_nodes, node_positions


def build_distance_weighted_graph(ds, distance_threshold=5.0):
    """
    Build graph with edges weighted by inverse distance.
    
    Args:
        ds: xarray Dataset
        distance_threshold: Maximum distance (degrees) for connections
    
    Returns:
        edge_index, edge_weights, num_nodes
    """
    lats = ds.latitude.values
    lons = ds.longitude.values
    
    lat_grid, lon_grid = np.meshgrid(lats, lons, indexing='ij')
    node_positions = np.c_[lat_grid.ravel(), lon_grid.ravel()]
    num_nodes = len(node_positions)
    
    edges = []
    weights = []
    
    for i in range(num_nodes):
        for j in range(i+1, num_nodes):
            dist = np.linalg.norm(node_positions[i] - node_positions[j])
            if dist < distance_threshold and dist > 0:
                edges.append([i, j])
                edges.append([j, i])  # Undirected
                weight = 1.0 / dist
                weights.append(weight)
                weights.append(weight)
    
    edge_index = torch.tensor(edges, dtype=torch.long).t()
    edge_weights = torch.tensor(weights, dtype=torch.float32)
    
    return edge_index, edge_weights, num_nodes


######################################## END OF graphBuilder.py ########################################


============================================================
FILE 8/12: maml.py
============================================================

"""
Model-Agnostic Meta-Learning (MAML) implementation.
"""
import torch
import torch.nn as nn
import torch.optim as optim

try:
    import higher
    HIGHER_AVAILABLE = True
except ImportError:
    HIGHER_AVAILABLE = False
    print("Warning: 'higher' library not found. Install with: pip install higher")


class MAMLTrainer:
    """
    MAML trainer for weather forecasting.
    """
    
    def __init__(self, model, inner_lr=0.01, outer_lr=0.001, inner_steps=5):
        """
        Args:
            model: ST-GCN model
            inner_lr: Learning rate for inner loop (task adaptation)
            outer_lr: Learning rate for outer loop (meta-update)
            inner_steps: Number of gradient steps in inner loop
        """
        self.model = model
        self.inner_lr = inner_lr
        self.outer_lr = outer_lr
        self.inner_steps = inner_steps
        
        self.meta_optimizer = optim.Adam(model.parameters(), lr=outer_lr)
        self.criterion = nn.MSELoss()
    
    def train_step(self, tasks):
        """
        Single meta-training step across all tasks.
        
        Args:
            tasks: List of WeatherTask objects
        
        Returns:
            meta_loss: Average loss across tasks
        """
        if not HIGHER_AVAILABLE:
            raise RuntimeError("'higher' library required. Install with: pip install higher")
        
        self.meta_optimizer.zero_grad()
        meta_loss = 0.0
        
        for task in tasks:
            # Create differentiable optimizer for inner loop
            with higher.innerloop_ctx(
                self.model, 
                optim.SGD(self.model.parameters(), lr=self.inner_lr),
                copy_initial_weights=False
            ) as (fmodel, diffopt):
                
                # Inner loop: adapt on support set
                for step in range(self.inner_steps):
                    for batch in task.support_loader:
                        support_loss = self.criterion(
                            fmodel(batch.x, batch.edge_index).squeeze(), 
                            batch.y
                        )
                        diffopt.step(support_loss)
                        break  # One batch per step
                
                # Outer loop: evaluate on query set
                query_loss = 0.0
                num_batches = 0
                
                for batch in task.query_loader:
                    logits = fmodel(batch.x, batch.edge_index).squeeze()
                    loss = self.criterion(logits, batch.y)
                    query_loss += loss
                    num_batches += 1
                
                query_loss = query_loss / num_batches
                meta_loss += query_loss
        
        # Average and backprop meta-loss
        meta_loss = meta_loss / len(tasks)
        meta_loss.backward()
        self.meta_optimizer.step()
        
        return meta_loss.item()


######################################## END OF maml.py ########################################


============================================================
FILE 9/12: model.py
============================================================

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv


class STGCN(nn.Module):
    def __init__(
        self,
        in_channels,
        hidden_channels,
        out_channels=12,
        window_size=6,
        forecast_horizon=1,
        dropout_rate=0.3,
    ):
        super(STGCN, self).__init__()
        self.window_size = window_size
        self.out_channels = out_channels
        self.forecast_horizon = forecast_horizon
        self.dropout_rate = dropout_rate

        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.conv3 = GCNConv(hidden_channels, hidden_channels)
        self.conv4 = GCNConv(hidden_channels, hidden_channels)
        self.dropout = nn.Dropout(p=dropout_rate)
        self.output_layer = nn.Linear(hidden_channels, out_channels * forecast_horizon)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.conv2(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.conv3(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.conv4(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)

        # Output for last window for all nodes
        num_nodes = (
            x.shape[0] // self.window_size
        )  # <-- Double check this matches WeatherGraphDataset!
        x = x[-num_nodes:]  # [num_nodes, hidden_channels]
        x = self.output_layer(x)
        x = x.view(num_nodes, self.forecast_horizon, self.out_channels)
        x = x.reshape(-1, self.out_channels)
        return x


######################################## END OF model.py ########################################


============================================================
FILE 10/12: temp.py
============================================================

import torch
import xarray as xr
import numpy as np
import os
import csv

from model import STGCN
from graphBuilder import build_spatial_graph
from featurePreprocessor import prepare_model_input
from dataset import WeatherGraphDataset
from embed_utils import add_time_embeddings, KoppenEmbedding

MODEL_PATH = "./Out_Data/AdaptedModels/adapted_model_2021_(-40, -35, 140, 145).pt"
DATA_DIR = r"E:\\Study\\5th Sem Mini Project\\Datasets\\2025\\Jan2Mar"
ACCUM_FILE = os.path.join(DATA_DIR, "data_stream-oper_stepType-accum.nc")
INSTANT_FILE = os.path.join(DATA_DIR, "data_stream-oper_stepType-instant.nc")
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

lat_min, lat_max = -40, -35
lon_min, lon_max = 140, 145


def wrap_lon(lon):
    return lon if lon >= 0 else lon + 360


wrapped_lon_min = wrap_lon(lon_min)
wrapped_lon_max = wrap_lon(lon_max)

WINDOW_SIZE = 24
FORECAST_HORIZON = 8
TIME_WINDOW = WINDOW_SIZE + FORECAST_HORIZON + 8

print("Loading model checkpoint:", MODEL_PATH)
checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)
config = checkpoint["config"]

model = STGCN(
    in_channels=config["input_channels"],
    hidden_channels=config["hidden_channels"],
    out_channels=config["output_channels"],
    window_size=config["window_size"],
    forecast_horizon=config["forecast_horizon"],
    dropout_rate=0.3,
).to(DEVICE)
koppen_embed = KoppenEmbedding(embedding_dim=8).to(DEVICE)
model.load_state_dict(checkpoint["model_state_dict"])
koppen_embed.load_state_dict(checkpoint["koppen_embed_state_dict"])
model.eval()
koppen_embed.eval()
print("Model loaded and set to eval.")

ds_accum = xr.open_dataset(ACCUM_FILE)
ds_instant = xr.open_dataset(INSTANT_FILE)
ds = xr.merge([ds_accum, ds_instant])
lat_name = "latitude"
lon_name = "longitude"
time_name = "valid_time"

ds_reg = ds.sel(
    **{
        lat_name: slice(lat_max, lat_min),
        lon_name: slice(wrapped_lon_min, wrapped_lon_max),
    }
)
if ds_reg[lat_name].size == 0 or ds_reg[lon_name].size == 0:
    print("ERROR: region crop returns no gridpoints! Check coverage and fix bounds.")
    exit(1)
if time_name in ds_reg.coords:
    ds_reg = ds_reg.isel({time_name: slice(0, TIME_WINDOW)})
else:
    print("valid_time not found!")
    exit(1)
if "day_of_year_sin" not in ds_reg:
    ds_reg = add_time_embeddings(ds_reg)

edge_index, num_nodes, _ = build_spatial_graph(ds_reg, k_neighbors=4)
koppen_code = checkpoint.get("koppen_code", 0) if "koppen_code" in checkpoint else 0
features, stats = prepare_model_input(ds_reg, koppen_code, koppen_embed, normalize=True)
needed_timesteps = WINDOW_SIZE + FORECAST_HORIZON
print(
    f"Needed timesteps for one sample: {needed_timesteps} (you have {features.shape[0]})"
)

dataset = WeatherGraphDataset(
    features, edge_index, window_size=WINDOW_SIZE, forecast_horizon=FORECAST_HORIZON
)
print("WeatherGraphDataset size:", len(dataset))
if len(dataset) == 0:
    print("ERROR: Not enough timesteps for even one sample. Increase TIME_WINDOW.")
    exit(1)

if isinstance(stats, dict):
    mean = np.array(stats["mean"])
    std = np.array(stats["std"])
else:
    mean, std = stats

VAR_NAMES = [
    "u10",
    "v10",
    "t2m",
    "d2m",
    "sp",
    "tp",
    "u100",
    "v100",
    "str",
    "hcc",
    "lcc",
    "e",
]  # Update if your order is different!
nvars = len(VAR_NAMES)

# --- Generate timeline-format CSV
timeline_save = "./Out_Data/timeline_window_allvars.csv"

with open(timeline_save, "w", newline="") as csvfile:
    writer = csv.writer(csvfile)
    header = ["TIMESTAMP"]
    for var in VAR_NAMES:
        header += [
            f"{var} TRUE NORM",
            f"{var} TRUE PHYS",
            f"{var} PRED NORM",
            f"{var} PRED PHYS",
        ]
    writer.writerow(header)

    for idx in range(len(dataset)):
        window = dataset[idx]
        x_cpu = window.x.cpu().numpy()  # [window, nodes, nvars] or [window, nvars]
        y_true = window.y.cpu().numpy()  # [nodes, horizon, nvars]
        with torch.no_grad():
            y_pred = (
                model(window.x.to(DEVICE), window.edge_index.to(DEVICE)).cpu().numpy()
            )  # [nodes, horizon, nvars]

        # Normalize shapes
        if len(x_cpu.shape) == 3:
            x_avg = x_cpu.mean(axis=1)  # [window, nvars]
        else:
            x_avg = x_cpu  # [window, nvars]
        y_true_avg = y_true.mean(axis=0)  # [horizon, nvars]
        y_pred_avg = y_pred.mean(axis=0)  # [horizon, nvars]

        input_timesteps = ds_reg[time_name].values[idx : idx + WINDOW_SIZE]
        forecast_timesteps = ds_reg[time_name].values[
            idx + WINDOW_SIZE : idx + WINDOW_SIZE + FORECAST_HORIZON
        ]

        # --- History rows (input, no prediction)
        for t_idx, ts in enumerate(input_timesteps):
            row = [str(ts)]
            for v_idx in range(nvars):
                true_norm = x_avg[t_idx, v_idx]
                true_phys = true_norm * std[v_idx] + mean[v_idx]
                row += [f"{true_norm:.4f}", f"{true_phys:.4f}", "", ""]
            writer.writerow(row)

        # --- Prediction rows (forecast)
        for t_idx, ts in enumerate(forecast_timesteps):
            row = [str(ts)]
            for v_idx in range(nvars):
                true_norm = y_true_avg[t_idx, v_idx]
                true_phys = true_norm * std[v_idx] + mean[v_idx]
                pred_norm = y_pred_avg[t_idx, v_idx]
                pred_phys = pred_norm * std[v_idx] + mean[v_idx]
                row += [
                    f"{true_norm:.4f}",
                    f"{true_phys:.4f}",
                    f"{pred_norm:.4f}",
                    f"{pred_phys:.4f}",
                ]
            writer.writerow(row)

print(f"(Saved: {timeline_save})")


######################################## END OF temp.py ########################################


============================================================
FILE 11/12: train_maml_optimized.py
============================================================

import torch
import torch.nn as nn
import numpy as np
import copy
import time
import os
import xarray as xr
import random
from torch_geometric.loader import DataLoader
from torch.utils.data import random_split, Subset
from dataLoader import main_dataloader
from embed_utils import add_time_embeddings, KoppenEmbedding
from graphBuilder import build_spatial_graph
from featurePreprocessor import prepare_model_input, WEATHER_VARS
from dataset import WeatherGraphDataset
from model import STGCN

# ========== Reproducibility ==========
SEED = 42
np.random.seed(SEED)
random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

# ========== CONFIGURATION ==========
NUM_EPOCHS = 50
BATCH_SIZE = 3  # Number of meta-tasks per batch, NOT node batch!
INNER_BATCH_SIZE = 1  # Must be 1 for shape-stable processing!
INNER_EPOCHS_PER_TASK = 3
INNER_LR = 0.005
OUTER_LR = 0.0005
MAX_GRAD_NORM = 1.0
WEIGHT_DECAY = 1e-4
WINDOW_SIZE = 24
FORECAST_HORIZON = 8
NUM_WEATHER_VARS = 12

KOPPEN_EMBED_DIM = 8
INPUT_CHANNELS = NUM_WEATHER_VARS + 4 + KOPPEN_EMBED_DIM
HIDDEN_CHANNELS = 128
OUTPUT_CHANNELS = NUM_WEATHER_VARS

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

META_TRAIN_REGIONS = [
    (-9.5, -4.5, -67.5, -62.5),
    (12.5, 17.5, 102.5, 107.5),
    (22.5, 27.5, 19.5, 24.5),
    (-23.5, -18.5, 132.5, 137.5),
    (43.5, 48.5, 7.5, 12.5),
    (35.5, 40.5, -5.5, -0.5),
    (53.5, 58.5, 34.5, 39.5),
    (44.5, 49.5, 125.5, 130.5),
    (67.5, 72.5, -32.5, -27.5),
    (-20, -15, -70, -65),
    (32.5, 37.5, 137.5, 142.5),
    (-35.5, -30.5, 16.5, 21.5),
    (51.5, 56.5, -112.5, -107.5),
    (29.5, 34.5, -101.5, -96.5),
    (11.5, 16.5, 86.5, 91.5),
]

print("\n" + "=" * 80)
print("MAML TRAINING: MULTI-VARIABLE WEATHER FORECASTING")
print("FINAL OPTIMIZED VERSION WITH STABILITY & PERFORMANCE FIXES")
print("=" * 80)
print(f"Device: {DEVICE}")
print(f"Architecture: 4-layer GCN, {HIDDEN_CHANNELS} hidden")
print(f"Epochs: {NUM_EPOCHS}")
print(f"Inner LR: {INNER_LR}, Outer LR: {OUTER_LR}")
print(f"Gradient Clipping: {MAX_GRAD_NORM}")
print(f"Weight Decay: {WEIGHT_DECAY}")
print("=" * 80 + "\n")


# ========== DATA LOADING AND TASK CREATION ==========
def load_cached_or_create(region_bounds, koppen_embed):
    """
    Loads a region dataset from cache if available, else loads fresh and embeds time.
    """
    lat_min, lat_max, lon_min, lon_max = region_bounds
    cache_path = f"Out_Data/lat{lat_min}-{lat_max}_lon{lon_min}-{lon_max}.nc"
    if os.path.exists(cache_path):
        print(f"  ‚úÖ Loading cached: {cache_path}")
        ds = xr.open_dataset(cache_path)
        koppen_code = int(ds.attrs.get("koppen_code", 0))
    else:
        print(f"  ‚ö†Ô∏è  No cache, loading from scratch...")
        ds, koppen_code, _ = main_dataloader(lat_min, lat_max, lon_min, lon_max)
        ds = add_time_embeddings(ds)
    if "day_of_year_sin" not in ds:
        ds = add_time_embeddings(ds)
    return ds, koppen_code


def chronological_split(dataset, support_ratio=0.8, gap=0):
    """
    Chronological split with anti-leakage optional gap: [support][gap][query]
    """
    n = len(dataset)
    split = int(support_ratio * n)
    q_start = min(split + gap, n)
    support_idx = list(range(0, split))
    query_idx = list(range(q_start, n))
    return Subset(dataset, support_idx), Subset(dataset, query_idx)


def create_task(region_bounds, koppen_embed, window_size=WINDOW_SIZE, support_size=0.8):
    ds, koppen_code = load_cached_or_create(region_bounds, koppen_embed)
    edge_index, num_nodes, _ = build_spatial_graph(ds, k_neighbors=4)
    features, stats = prepare_model_input(ds, koppen_code, koppen_embed, normalize=True)
    dataset = WeatherGraphDataset(
        features, edge_index, window_size=window_size, forecast_horizon=FORECAST_HORIZON
    )
    support_len = int(support_size * len(dataset))
    support_ds, query_ds = chronological_split(
        dataset, support_ratio=0.8, gap=WINDOW_SIZE
    )
    return support_ds, query_ds, stats


# ========== INNER LOOP ==========
def inner_loop(model, koppen_embed, support_ds, inner_epochs, inner_lr, device):
    """
    Simulates fast adaptation (inner loop of MAML); only sees support set.
    """
    temp_model = copy.deepcopy(model)
    temp_koppen = copy.deepcopy(koppen_embed)
    temp_model.train()
    temp_koppen.train()
    optimizer = torch.optim.SGD(
        list(temp_model.parameters()) + list(temp_koppen.parameters()),
        lr=inner_lr,
        weight_decay=WEIGHT_DECAY,
    )
    criterion = nn.MSELoss()
    support_loader = DataLoader(
        support_ds,
        batch_size=1,
        shuffle=False,
        num_workers=0,
        pin_memory=(device == "cuda"),
    )
    losses = []
    for epoch in range(inner_epochs):
        for batch in support_loader:
            batch = batch.to(device)
            optimizer.zero_grad()
            out = temp_model(batch.x, batch.edge_index)
            loss = criterion(out, batch.y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(
                list(temp_model.parameters()) + list(temp_koppen.parameters()),
                max_norm=MAX_GRAD_NORM,
            )
            optimizer.step()
            losses.append(loss.item())
    return temp_model, temp_koppen, losses


# ========== OUTER LOOP ==========
def meta_update(model, koppen_embed, tasks, inner_epochs, inner_lr, outer_lr, device):
    """
    Performs the meta-update (outer loop of MAML): adapts per task,
    evaluates on one query batch per task (low memory, correct gradients).
    """
    meta_loss = 0.0
    criterion = nn.MSELoss()
    meta_optimizer = torch.optim.Adam(
        list(model.parameters()) + list(koppen_embed.parameters()),
        lr=outer_lr,
        weight_decay=WEIGHT_DECAY,
    )
    meta_optimizer.zero_grad()
    for support_ds, query_ds, stats in tasks:
        adapted_model, adapted_koppen, _ = inner_loop(
            model, koppen_embed, support_ds, inner_epochs, inner_lr, device
        )
        adapted_model.eval()
        query_loader = DataLoader(
            query_ds,
            batch_size=1,
            shuffle=False,
            num_workers=0,
            pin_memory=(device == "cuda"),
        )
        # Use ONLY the first batch in query set to compute meta-loss (with gradient tracking)
        query_batch = next(iter(query_loader))
        query_batch = query_batch.to(device)
        query_out = adapted_model(query_batch.x, query_batch.edge_index)
        query_loss = criterion(query_out, query_batch.y)
        meta_loss += query_loss
        del adapted_model, adapted_koppen
        torch.cuda.empty_cache()
    if len(tasks) > 0:
        meta_loss = meta_loss / len(tasks)
    meta_loss.backward()
    torch.nn.utils.clip_grad_norm_(
        list(model.parameters()) + list(koppen_embed.parameters()),
        max_norm=MAX_GRAD_NORM,
    )
    meta_optimizer.step()
    return meta_loss.item()


# ========== MAIN ==========
def main():
    print("Initializing model...")
    model = STGCN(
        in_channels=INPUT_CHANNELS,
        hidden_channels=HIDDEN_CHANNELS,
        out_channels=OUTPUT_CHANNELS,
        window_size=WINDOW_SIZE,
        forecast_horizon=FORECAST_HORIZON,
        dropout_rate=0.3,
    ).to(DEVICE)
    koppen_embed = KoppenEmbedding(embedding_dim=KOPPEN_EMBED_DIM).to(DEVICE)
    print(f"‚úÖ STGCN initialized:")
    print(f"   Input channels: {INPUT_CHANNELS}")
    print(f"   Hidden channels: {HIDDEN_CHANNELS}")
    print(f"   Output channels: {OUTPUT_CHANNELS}")
    print(f"   Window size: {WINDOW_SIZE}")
    print(f"   Forecast horizon: {FORECAST_HORIZON}")
    print(f"‚úÖ Total parameters: {sum(p.numel() for p in model.parameters()):,}\n")
    print("Loading tasks...")
    all_tasks = []
    for i, region in enumerate(META_TRAIN_REGIONS):
        print(f"\nRegion {i+1}: {region}")
        try:
            support_ds, query_ds, stats = create_task(region, koppen_embed)
            all_tasks.append((support_ds, query_ds, stats))
            print(f"  ‚úÖ Support: {len(support_ds)}, Query: {len(query_ds)}")
        except Exception as e:
            print(f"  ‚ùå Error: {e}")
            continue
    if not all_tasks:
        print("\n‚ùå No tasks loaded!")
        return
    print(f"\n‚úÖ Loaded {len(all_tasks)} tasks\n")
    print("=" * 80)
    print("STARTING TRAINING")
    print("=" * 80)
    best_loss = float("inf")
    start = time.time()
    DO_LOG = True
    log_file = "./Out_Data/maml_training_log.csv"
    if DO_LOG:
        with open(log_file, "w") as f:
            f.write("epoch,meta_loss\n")
    for epoch in range(NUM_EPOCHS):
        epoch_start = time.time()
        if len(all_tasks) > BATCH_SIZE:
            indices = np.random.choice(len(all_tasks), BATCH_SIZE, replace=False)
            batch_tasks = [all_tasks[i] for i in indices]
        else:
            batch_tasks = all_tasks
        loss = meta_update(
            model,
            koppen_embed,
            batch_tasks,
            INNER_EPOCHS_PER_TASK,
            INNER_LR,
            OUTER_LR,
            DEVICE,
        )
        epoch_time = time.time() - epoch_start
        print(
            f"Epoch {epoch+1}/{NUM_EPOCHS} - MetaLoss: {loss:.4f} - Time: {epoch_time:.1f}s"
        )
        if DO_LOG:
            with open(log_file, "a") as f:
                f.write(f"{epoch+1},{loss}\n")
        if loss < best_loss:
            best_loss = loss
            save_dir = "./Out_Data/SavedModels"
            os.makedirs(save_dir, exist_ok=True)
            save_path = os.path.join(
                save_dir, "maml_model_multivar_(LongerWindowSize&LRRate).pt"
            )
            torch.save(
                {
                    "model_state_dict": model.state_dict(),
                    "koppen_embed_state_dict": koppen_embed.state_dict(),
                    "meta_loss": best_loss,
                    "epoch": epoch,
                    "config": {
                        "input_channels": INPUT_CHANNELS,
                        "hidden_channels": HIDDEN_CHANNELS,
                        "output_channels": OUTPUT_CHANNELS,
                        "window_size": WINDOW_SIZE,
                        "forecast_horizon": FORECAST_HORIZON,
                        "inner_lr": INNER_LR,
                        "outer_lr": OUTER_LR,
                        "max_grad_norm": MAX_GRAD_NORM,
                        "weight_decay": WEIGHT_DECAY,
                    },
                },
                save_path,
            )
        # Extra safety: clear cache after each epoch
        if DEVICE == "cuda":
            torch.cuda.empty_cache()
    total = time.time() - start
    print("\n" + "=" * 80)
    print("COMPLETE!")
    print("=" * 80)
    print(f"Time: {total/3600:.2f} hours")
    print(f"Best loss achieved: {best_loss:.4f}")
    print("=" * 80)


if __name__ == "__main__":
    main()


######################################## END OF train_maml_optimized.py ########################################


============================================================
FILE 12/12: validateAdaptedModel.py
============================================================

import torch
import xarray as xr
import numpy as np
import os
import csv

from model import STGCN
from graphBuilder import build_spatial_graph
from featurePreprocessor import prepare_model_input
from dataset import WeatherGraphDataset
from embed_utils import add_time_embeddings, KoppenEmbedding

MODEL_PATH = "./Out_Data/AdaptedModels/adapted_model_2021_(-40, -35, 140, 145).pt"
DATA_DIR = r"E:\Study\5th Sem Mini Project\Datasets\2025\Jan2Mar"
ACCUM_FILE = os.path.join(DATA_DIR, "data_stream-oper_stepType-accum.nc")
INSTANT_FILE = os.path.join(DATA_DIR, "data_stream-oper_stepType-instant.nc")
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

lat_min, lat_max = -40, -35
lon_min, lon_max = 140, 145


def wrap_lon(lon):
    return lon if lon >= 0 else lon + 360


wrapped_lon_min = wrap_lon(lon_min)
wrapped_lon_max = wrap_lon(lon_max)

WINDOW_SIZE = 24
FORECAST_HORIZON = 8
TIME_WINDOW = WINDOW_SIZE + FORECAST_HORIZON + 8

print("Loading model checkpoint:", MODEL_PATH)
checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)
config = checkpoint["config"]

model = STGCN(
    in_channels=config["input_channels"],
    hidden_channels=config["hidden_channels"],
    out_channels=config["output_channels"],
    window_size=config["window_size"],
    forecast_horizon=config["forecast_horizon"],
    dropout_rate=0.3,
).to(DEVICE)
koppen_embed = KoppenEmbedding(embedding_dim=8).to(DEVICE)
model.load_state_dict(checkpoint["model_state_dict"])
koppen_embed.load_state_dict(checkpoint["koppen_embed_state_dict"])
model.eval()
koppen_embed.eval()
print("Model loaded and set to eval.")

ds_accum = xr.open_dataset(ACCUM_FILE)
ds_instant = xr.open_dataset(INSTANT_FILE)
ds = xr.merge([ds_accum, ds_instant])
lat_name = "latitude"
lon_name = "longitude"
time_name = "valid_time"

ds_reg = ds.sel(
    **{
        lat_name: slice(lat_max, lat_min),
        lon_name: slice(wrapped_lon_min, wrapped_lon_max),
    }
)
if ds_reg[lat_name].size == 0 or ds_reg[lon_name].size == 0:
    print("ERROR: region crop returns no gridpoints! Check coverage and fix bounds.")
    exit(1)
if time_name in ds_reg.coords:
    ds_reg = ds_reg.isel({time_name: slice(0, TIME_WINDOW)})
else:
    print("valid_time not found!")
    exit(1)
if "day_of_year_sin" not in ds_reg:
    ds_reg = add_time_embeddings(ds_reg)

edge_index, num_nodes, _ = build_spatial_graph(ds_reg, k_neighbors=4)
koppen_code = checkpoint.get("koppen_code", 0) if "koppen_code" in checkpoint else 0
features, stats = prepare_model_input(ds_reg, koppen_code, koppen_embed, normalize=True)
needed_timesteps = WINDOW_SIZE + FORECAST_HORIZON
print(
    f"Needed timesteps for one sample: {needed_timesteps} (you have {features.shape[0]})"
)

dataset = WeatherGraphDataset(
    features, edge_index, window_size=WINDOW_SIZE, forecast_horizon=FORECAST_HORIZON
)
print("WeatherGraphDataset size:", len(dataset))
if len(dataset) == 0:
    print("ERROR: Not enough timesteps for even one sample. Increase TIME_WINDOW.")
    exit(1)

# --- Get normalization stats for all variables
if isinstance(stats, dict):
    mean = np.array(stats["mean"])
    std = np.array(stats["std"])
else:
    mean, std = stats

VAR_NAMES = [
    "u10",
    "v10",
    "t2m",
    "d2m",
    "sp",
    "tp",
    "u100",
    "v100",
    "str",
    "hcc",
    "lcc",
    "e",
]  # Update if your order is different!
nvars = len(VAR_NAMES)

# ========== TIMELINE CSV (wide, timestamped) ==========
timeline_save = "./Out_Data/timeline_window_allvars.csv"
with open(timeline_save, "w", newline="") as csvfile:
    writer = csv.writer(csvfile)
    header = ["TIMESTAMP"]
    for var in VAR_NAMES:
        header += [
            f"{var} TRUE NORM",
            f"{var} TRUE PHYS",
            f"{var} PRED NORM",
            f"{var} PRED PHYS",
        ]
    writer.writerow(header)

    for idx in range(len(dataset)):
        window = dataset[idx]
        x_cpu = window.x.cpu().numpy()
        y_true = window.y.cpu().numpy()
        with torch.no_grad():
            y_pred = (
                model(window.x.to(DEVICE), window.edge_index.to(DEVICE)).cpu().numpy()
            )

        # Average over nodes for each window and forecast
        if len(x_cpu.shape) == 3:
            x_avg = x_cpu.mean(axis=1)  # [window, nvars]
        else:
            x_avg = x_cpu
        y_true_avg = y_true.mean(axis=0)  # [horizon, nvars]
        y_pred_avg = y_pred.mean(axis=0)  # [horizon, nvars]
        # Enforce shape for single-timestep cases
        if x_avg.ndim == 1:
            x_avg = x_avg[None, :]
        if y_true_avg.ndim == 1:
            y_true_avg = y_true_avg[None, :]
        if y_pred_avg.ndim == 1:
            y_pred_avg = y_pred_avg[None, :]

        input_timesteps = ds_reg[time_name].values[idx : idx + WINDOW_SIZE]
        forecast_timesteps = ds_reg[time_name].values[
            idx + WINDOW_SIZE : idx + WINDOW_SIZE + FORECAST_HORIZON
        ]

        # History window (no prediction)
        for t_idx, ts in enumerate(input_timesteps):
            if t_idx >= x_avg.shape[0]:
                continue
            row = [str(ts)]
            for v_idx in range(nvars):
                true_norm = x_avg[t_idx, v_idx]
                true_phys = true_norm * std[v_idx] + mean[v_idx]
                row += [f"{true_norm:.4f}", f"{true_phys:.4f}", "", ""]
            writer.writerow(row)

        # Forecast window (truth and prediction)
        for t_idx, ts in enumerate(forecast_timesteps):
            if t_idx >= y_true_avg.shape[0] or t_idx >= y_pred_avg.shape[0]:
                continue
            row = [str(ts)]
            for v_idx in range(nvars):
                true_norm = y_true_avg[t_idx, v_idx]
                true_phys = true_norm * std[v_idx] + mean[v_idx]
                pred_norm = y_pred_avg[t_idx, v_idx]
                pred_phys = pred_norm * std[v_idx] + mean[v_idx]
                row += [
                    f"{true_norm:.4f}",
                    f"{true_phys:.4f}",
                    f"{pred_norm:.4f}",
                    f"{pred_phys:.4f}",
                ]
            writer.writerow(row)

print(f"(Saved: {timeline_save})")

# ========== RMSE / BIAS SUMMARY ==========
all_preds_phys = [[] for _ in range(nvars)]
all_trues_phys = [[] for _ in range(nvars)]
for idx in range(len(dataset)):
    window = dataset[idx]
    x = window.x.to(DEVICE)
    edge_idx = window.edge_index.to(DEVICE)
    y_true = window.y.cpu().numpy()
    with torch.no_grad():
        y_pred = model(x, edge_idx).cpu().numpy()
    # Reshape if needed
    if y_true.ndim == 2:
        y_true = y_true.reshape(num_nodes, FORECAST_HORIZON, nvars)
    if y_pred.ndim == 2:
        y_pred = y_pred.reshape(num_nodes, FORECAST_HORIZON, nvars)
    # Compute RMSE/bias only for forecast window
    for t in range(min(FORECAST_HORIZON, y_pred.shape[1])):
        for v_idx in range(nvars):
            mean_pred_phys = y_pred[:, t, v_idx].mean() * std[v_idx] + mean[v_idx]
            mean_true_phys = y_true[:, t, v_idx].mean() * std[v_idx] + mean[v_idx]
            all_preds_phys[v_idx].append(mean_pred_phys)
            all_trues_phys[v_idx].append(mean_true_phys)

print(
    "\n====== RMSE and Bias Statistics (Phys Units, averaged over all nodes, all samples, all horizons) ======"
)
for v_idx, var in enumerate(VAR_NAMES):
    arr_pred = np.array(all_preds_phys[v_idx])
    arr_true = np.array(all_trues_phys[v_idx])
    rmse = np.sqrt(np.mean((arr_pred - arr_true) ** 2))
    bias = np.mean(arr_pred - arr_true)
    print(
        f"{var}:\tRMSE = {rmse:.4f}\tbias = {bias:.4f}\tTrue avg = {arr_true.mean():.4f}\tPred avg = {arr_pred.mean():.4f}"
    )
print(
    "====================================================================================="
)


######################################## END OF validateAdaptedModel.py ########################################

